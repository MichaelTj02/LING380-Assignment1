{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name (Last, First) | Student ID  | Section contributed                     | Section edited     | Other contributions                     |\n",
    "|--------------------|------------|------------------------------------------|--------------------|-----------------------------------------|\n",
    "| Tjokrowardojo, Michael      | 301416843   | Data collection, GitHub syncing         | Code and write-up | Troubleshoot spaCy                     |\n",
    "| Ng, Ryan     | 3014423701   | Data Collection, Function for most frequent words        | Code              | Communication and task coordination    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "- Podcast: https://www.youtube.com/watch?v=Ex3OQhsVaiY\n",
    "- Reading: https://www.gutenberg.org/ebooks/2160\n",
    "- Bee Movie Script: https://gist.github.com/MattIPv4/045239bc27b16b2bcf7a3a9a4648c08a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to read and process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(path):\n",
    "    \"\"\"\n",
    "    Scans a directory for text files and processes their content using spaCy and stores them in a doc object.\n",
    "\n",
    "    Args: \n",
    "        path (str): The directory path containing text files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where file names are keys and the processed text data (tokens, types, lexical diversity) are values.\n",
    "    \n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):    \n",
    "            file_path = os.path.join(path, filename)      \n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                file_info[filename] = nlp(text)\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Process Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data'\n",
    "\n",
    "files_in_dir_info = process_dir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Tokens and Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: BeeMovieScript.txt\n",
      "\n",
      "Number of tokens: 12687\n",
      "Lexical Diversity: 0.16347442263734532\n",
      "\n",
      "========================================\n",
      "\n",
      "File: Podcast.txt\n",
      "\n",
      "Number of tokens: 28052\n",
      "Lexical Diversity: 0.11678311706830173\n",
      "\n",
      "========================================\n",
      "\n",
      "File: TheExpeditionofHumphryClinker.txt\n",
      "\n",
      "Number of tokens: 191955\n",
      "Lexical Diversity: 0.07493943893099946\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the dictionary to process tokens and types for each file\n",
    "for filename, doc in files_in_dir_info.items():\n",
    "    # Extract tokens and types\n",
    "    tokens = [token.text for token in doc]\n",
    "    types = set(tokens)\n",
    "    \n",
    "    # Compute lexical diversity\n",
    "    lexical_diversity = len(types) / len(tokens) if len(tokens) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "    # Print file name\n",
    "    print(f\"File: {filename}\")\n",
    "\n",
    "    # Print number of tokens and types\n",
    "    print(\"\\nNumber of tokens:\", len(tokens))\n",
    "    print(\"Lexical Diversity:\", lexical_diversity)\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Most Frequent Words (Not Complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: BeeMovieScript.txt\n",
      "\n",
      "10 Most Common Words (No Punctuation):\n",
      "i: 349\n",
      "you: 327\n",
      "a: 259\n",
      "the: 259\n",
      "it: 239\n",
      "s: 214\n",
      "to: 191\n",
      "nt: 139\n",
      "that: 138\n",
      "of: 134\n",
      "\n",
      "========================================\n",
      "\n",
      "File: Podcast.txt\n",
      "\n",
      "10 Most Common Words (No Punctuation):\n",
      "the: 948\n",
      "you: 831\n",
      "to: 760\n",
      "i: 746\n",
      "s: 705\n",
      "and: 681\n",
      "that: 661\n",
      "a: 509\n",
      "it: 480\n",
      "of: 391\n",
      "\n",
      "========================================\n",
      "\n",
      "File: TheExpeditionofHumphryClinker.txt\n",
      "\n",
      "10 Most Common Words (No Punctuation):\n",
      "the: 8611\n",
      "of: 5674\n",
      "and: 5066\n",
      "to: 4442\n",
      "a: 3737\n",
      "in: 3205\n",
      "i: 2675\n",
      "he: 2109\n",
      "his: 2009\n",
      "that: 1897\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to remove punctuation\n",
    "def clean_word(word):\n",
    "    return ''.join(char for char in word if char.isalnum())  # Keeps only letters and numbers\n",
    "\n",
    "# Loop through the dictionary to count word frequencies in each file\n",
    "for filename, doc in files_in_dir_info.items():\n",
    "    # Extract tokens and clean them (remove punctuation)\n",
    "    tokens = [clean_word(token.text.lower()) for token in doc if clean_word(token.text)]  # Convert to lowercase and remove empty strings\n",
    "\n",
    "    # Count occurrences of each word using a dictionary\n",
    "    word_counts = {}\n",
    "    for word in tokens:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1  # Increment count\n",
    "\n",
    "    # Get the 10 most common words by sorting\n",
    "    most_common_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Print file name\n",
    "    print(f\"File: {filename}\")\n",
    "\n",
    "    # Print 10 most common words\n",
    "    print(\"\\n10 Most Common Words (No Punctuation):\")\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Named Entities Using SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
